{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fILeiDN-AEbs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install rasterio folium geopandas shapely plotly kagglehub #necessary imports, some not present in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MbWp2aR_2Ck"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UkQLRBTACAL"
      },
      "outputs": [],
      "source": [
        "# Set up the environment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnX6kvXjBPPN"
      },
      "outputs": [],
      "source": [
        "# Download the MARIDA dataset from the kaggle\n",
        "print(\"Downloading MARIDA dataset from Kaggle...\")\n",
        "path = kagglehub.dataset_download(\"anangfath/marida-marine-debrish-dataset\")\n",
        "print(f\"Path to dataset files: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eabSMS7XBSbV"
      },
      "outputs": [],
      "source": [
        "# Extract the dataset as it's in a zip file\n",
        "if os.path.exists(f\"{path}/marida.zip\"):\n",
        "    print(\"Extracting MARIDA dataset...\")\n",
        "    with zipfile.ZipFile(f\"{path}/marida.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(path)\n",
        "    print(\"Dataset extracted!\")\n",
        "\n",
        "# Defining our base directory\n",
        "base_dir = path\n",
        "\n",
        "# Create a directory for processed data\n",
        "processed_dir = os.path.join(base_dir, \"processed\")\n",
        "os.makedirs(processed_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VFcNbclBkfa"
      },
      "outputs": [],
      "source": [
        "# Define class names from the MARIDA repo\n",
        "class_mapping = {\n",
        "    1: 'Marine Debris',\n",
        "    2: 'Dense Sargassum',\n",
        "    3: 'Sparse Sargassum',\n",
        "    4: 'Natural Organic Material',\n",
        "    5: 'Ship',\n",
        "    6: 'Clouds',\n",
        "    7: 'Marine Water',\n",
        "    8: 'Sediment-Laden Water',\n",
        "    9: 'Foam',\n",
        "    10: 'Turbid Water',\n",
        "    11: 'Shallow Water',\n",
        "    12: 'Waves',\n",
        "    13: 'Cloud Shadows',\n",
        "    14: 'Wakes',\n",
        "    15: 'Mixed Water'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyAtrvnRBsMD"
      },
      "outputs": [],
      "source": [
        "# Define color mapping for visualization (used Deep seek for this)\n",
        "color_mapping = {\n",
        "    1: (255, 0, 0),      # Marine Debris - Red\n",
        "    2: (0, 128, 0),      # Dense Sargassum - Green\n",
        "    3: (144, 238, 144),  # Sparse Sargassum - Light Green\n",
        "    4: (139, 69, 19),    # Natural Organic Material - Brown\n",
        "    5: (128, 128, 128),  # Ship - Gray\n",
        "    6: (255, 255, 255),  # Clouds - White\n",
        "    7: (0, 0, 255),      # Marine Water - Blue\n",
        "    8: (210, 180, 140),  # Sediment-Laden Water - Tan\n",
        "    9: (255, 255, 224),  # Foam - Light Yellow\n",
        "    10: (64, 224, 208),  # Turbid Water - Turquoise\n",
        "    11: (176, 224, 230), # Shallow Water - Powder Blue\n",
        "    12: (0, 191, 255),   # Waves - Deep Sky Blue\n",
        "    13: (105, 105, 105), # Cloud Shadows - Dim Gray\n",
        "    14: (220, 220, 220), # Wakes - Gainsboro\n",
        "    15: (70, 130, 180)   # Mixed Water - Steel Blue\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhy0HedRByDV"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset_structure():\n",
        "    \"\"\"Analyze the dataset structure and print basic statistics\"\"\"\n",
        "    # Path to patches directory\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    if not os.path.exists(patches_dir):\n",
        "        print(f\"Error: Patches directory not found at {patches_dir}\")\n",
        "        return None\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    print(f\"Total number of S2 date-tile directories: {len(s2_dirs)}\")\n",
        "\n",
        "    # Count different types of files\n",
        "    image_count = 0\n",
        "    mask_count = 0\n",
        "    conf_count = 0\n",
        "\n",
        "    # Extract information from filenames\n",
        "    tiles = []\n",
        "    dates = []\n",
        "    locations = []\n",
        "\n",
        "    # Extract date and tile information\n",
        "    for s2_dir in tqdm(s2_dirs, desc=\"Analyzing directories\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "        files = os.listdir(dir_path)\n",
        "\n",
        "        # Count file types\n",
        "        images = [f for f in files if not f.endswith('_cl.tif') and not f.endswith('_conf.tif') and f.endswith('.tif')]\n",
        "        masks = [f for f in files if f.endswith('_cl.tif')]\n",
        "        confs = [f for f in files if f.endswith('_conf.tif')]\n",
        "\n",
        "        image_count += len(images)\n",
        "        mask_count += len(masks)\n",
        "        conf_count += len(confs)\n",
        "\n",
        "        \"\"\" Extract date and tile information - FIXED PATTERN MATCHING\n",
        "            The directory structure might be different than expected\n",
        "            Handle the case when dates list might be empty \"\"\"\n",
        "        try:\n",
        "            # Try different pattern formats that might exist in your data\n",
        "            # Original pattern: r'S2_(\\d{8})_(\\w+)'\n",
        "            # Modified patterns:\n",
        "            match = re.match(r'S2_(\\d{8})_(\\w+)', s2_dir)  # Original pattern\n",
        "            if not match:\n",
        "                match = re.match(r'(\\d{8})_(\\w+)', s2_dir)  # Try without 'S2_'\n",
        "            if not match:\n",
        "                match = re.match(r'S2_(\\d{8})', s2_dir)  # Try without tile information\n",
        "            if not match:\n",
        "                match = re.match(r'(\\d{8})', s2_dir)  # Try only date\n",
        "\n",
        "            if match:\n",
        "                date_str = match.group(1)  # Always get the first group (date)\n",
        "                date = datetime.strptime(date_str, '%Y%m%d')\n",
        "                dates.append(date)\n",
        "\n",
        "                if len(match.groups()) > 1:\n",
        "                    tile = match.group(2)  # Get tile information if available\n",
        "                    tiles.append(tile)\n",
        "\n",
        "        except (ValueError, IndexError) as e:\n",
        "            print(f\"Warning: Could not extract date or tile from '{s2_dir}': {e}\")\n",
        "\n",
        "    # Check if dates list is still empty after trying different patterns\n",
        "    if not dates:\n",
        "        print(\"Warning: Could not extract dates from directory names after trying different patterns\")\n",
        "        # Print directory names for debugging\n",
        "        print(f\"Sample directory names: {s2_dirs[:5] if len(s2_dirs) >= 5 else s2_dirs}\")\n",
        "        print(\"Date range: Unknown\")\n",
        "    else:\n",
        "        print(f\"Date range: {min(dates).strftime('%Y-%m-%d')} to {max(dates).strftime('%Y-%m-%d')}\")\n",
        "\n",
        "\n",
        "    print(f\"Total number of image patches: {image_count}\")\n",
        "    print(f\"Total number of classification masks: {mask_count}\")\n",
        "    print(f\"Total number of confidence level masks: {conf_count}\")\n",
        "\n",
        "    # Analyze train/val/test splits\n",
        "    splits_dir = os.path.join(base_dir, \"splits\")\n",
        "    if os.path.exists(splits_dir):\n",
        "        split_files = [f for f in os.listdir(splits_dir) if f.endswith('.txt')]\n",
        "        print(\"\\nTrain/Val/Test splits:\")\n",
        "        for split_file in split_files:\n",
        "            with open(os.path.join(splits_dir, split_file), 'r') as f:\n",
        "                samples = f.readlines()\n",
        "            print(f\"  {split_file}: {len(samples)} samples\")\n",
        "\n",
        "    # Analyze labels mapping for multi-label classification\n",
        "    labels_mapping_file = os.path.join(base_dir, \"labels_mapping.txt\")\n",
        "    if os.path.exists(labels_mapping_file):\n",
        "        with open(labels_mapping_file, 'r') as f:\n",
        "            label_lines = f.readlines()\n",
        "        print(f\"\\nMulti-label classification mapping: {len(label_lines)} entries\")\n",
        "\n",
        "    return {\n",
        "        'image_count': image_count,\n",
        "        'mask_count': mask_count,\n",
        "        'conf_count': conf_count,\n",
        "        'date_range': (min(dates), max(dates)) if dates else (None, None) # Handle empty dates\n",
        "    }\n",
        "\n",
        "dataset_stats = analyze_dataset_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yuYgmL8B4TV"
      },
      "outputs": [],
      "source": [
        "# Class Distribution Analysis\n",
        "\n",
        "def analyze_class_distribution(sample_size=50):\n",
        "    \"\"\"Analyze the distribution of classes in the dataset\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Initialize class counts\n",
        "    class_counts = {i: 0 for i in range(1, 16)}\n",
        "    total_pixels = 0\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Analyzing class distribution\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "        mask_files = [f for f in os.listdir(dir_path) if f.endswith('_cl.tif')]\n",
        "\n",
        "        for mask_file in mask_files:\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "            with rasterio.open(mask_path) as src:\n",
        "                mask_data = src.read(1)\n",
        "\n",
        "                # Count class pixels\n",
        "                for class_id in range(1, 16):\n",
        "                    class_counts[class_id] += np.sum(mask_data == class_id)\n",
        "\n",
        "                total_pixels += mask_data.size\n",
        "\n",
        "    # Calculate percentages\n",
        "    class_percentages = {class_id: count / total_pixels * 100\n",
        "                         for class_id, count in class_counts.items()}\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Class distribution (based on {sample_size} random samples):\")\n",
        "    for class_id, percentage in sorted(class_percentages.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {class_id}: {class_mapping[class_id]} - {percentage:.2f}%\")\n",
        "\n",
        "    # Plot class distribution\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    plt.bar(\n",
        "        [class_mapping[i] for i in range(1, 16)],\n",
        "        [class_percentages[i] for i in range(1, 16)]\n",
        "    )\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.ylabel(\"Percentage of Pixels\")\n",
        "    plt.title(\"Class Distribution in MARIDA Dataset\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(processed_dir, \"class_distribution.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate debris vs non-debris\n",
        "    debris_percentage = class_percentages[1]\n",
        "    non_debris_percentage = sum([p for i, p in class_percentages.items() if i != 1])\n",
        "\n",
        "    print(f\"\\nDebris vs. Non-debris:\")\n",
        "    print(f\"  Marine Debris: {debris_percentage:.2f}%\")\n",
        "    print(f\"  Non-debris: {non_debris_percentage:.2f}%\")\n",
        "\n",
        "    # Plot debris vs non-debris\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.pie([debris_percentage, non_debris_percentage],\n",
        "            labels=['Marine Debris', 'Non-debris'],\n",
        "            autopct='%1.1f%%',\n",
        "            colors=['#FF5733', '#3374FF'])\n",
        "    plt.title(\"Marine Debris vs. Non-debris Distribution\")\n",
        "    plt.savefig(os.path.join(processed_dir, \"debris_vs_non_debris.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return class_percentages\n",
        "\n",
        "class_distribution = analyze_class_distribution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfRrWYBIFCyw"
      },
      "outputs": [],
      "source": [
        "# Spectral Analysis\n",
        "\n",
        "def extract_spectral_signatures(sample_size=20):\n",
        "    \"\"\"Extract spectral signatures for different classes\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Initialize spectral signatures dictionary\n",
        "    spectral_signatures = {class_id: [] for class_id in range(1, 16)}\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Extracting spectral signatures\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find image-mask pairs\n",
        "        image_files = [f for f in os.listdir(dir_path) if not f.endswith('_cl.tif') and not f.endswith('_conf.tif') and f.endswith('.tif')]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(dir_path, image_file)\n",
        "            mask_file = image_file.replace('.tif', '_cl.tif')\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            # Open image and mask\n",
        "            with rasterio.open(image_path) as img_src, rasterio.open(mask_path) as mask_src:\n",
        "                img_data = img_src.read()\n",
        "                mask_data = mask_src.read(1)\n",
        "\n",
        "                # For each class, extract spectral signatures\n",
        "                for class_id in range(1, 16):\n",
        "                    class_pixels = np.where(mask_data == class_id)\n",
        "\n",
        "                    if len(class_pixels[0]) > 0:\n",
        "                        # Sample up to 100 pixels per class per image\n",
        "                        sample_indices = np.random.choice(len(class_pixels[0]),\n",
        "                                                          min(100, len(class_pixels[0])),\n",
        "                                                          replace=False)\n",
        "\n",
        "                        for idx in sample_indices:\n",
        "                            pixel_y, pixel_x = class_pixels[0][idx], class_pixels[1][idx]\n",
        "                            spectral_signature = img_data[:, pixel_y, pixel_x]\n",
        "                            spectral_signatures[class_id].append(spectral_signature)\n",
        "\n",
        "    # Calculate mean spectral signatures\n",
        "    mean_signatures = {}\n",
        "    for class_id, signatures in spectral_signatures.items():\n",
        "        if signatures:\n",
        "            mean_signatures[class_id] = np.mean(signatures, axis=0)\n",
        "\n",
        "    # Plot mean spectral signatures\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    for class_id, signature in mean_signatures.items():\n",
        "        plt.plot(signature, label=f\"{class_id}: {class_mapping[class_id]}\")\n",
        "\n",
        "    plt.xlabel(\"Sentinel-2 Band\")\n",
        "    plt.ylabel(\"Reflectance\")\n",
        "    plt.title(\"Mean Spectral Signatures by Class\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(processed_dir, \"spectral_signatures.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Focus on debris vs other classes\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Plot debris\n",
        "    if 1 in mean_signatures:\n",
        "        plt.plot(mean_signatures[1], 'r-', linewidth=3, label=f\"1: {class_mapping[1]}\")\n",
        "\n",
        "    for class_id, signature in mean_signatures.items():\n",
        "        plt.plot(signature, label=f\"{class_id}: {class_mapping[class_id]}\")\n",
        "\n",
        "    plt.xlabel(\"Sentinel-2 Band\")\n",
        "    plt.ylabel(\"Reflectance\")\n",
        "    plt.title(\"Mean Spectral Signatures by Class\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(processed_dir, \"spectral_signatures.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Focus on debris vs other classes\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Plot debris\n",
        "    if 1 in mean_signatures:\n",
        "        plt.plot(mean_signatures[1], 'r-', linewidth=3, label=f\"1: {class_mapping[1]}\")\n",
        "\n",
        "    # Plot other classes\n",
        "    for class_id, signature in mean_signatures.items():\n",
        "        if class_id != 1:  # Skip debris\n",
        "            plt.plot(signature, '--', alpha=0.7, label=f\"{class_id}: {class_mapping[class_id]}\")\n",
        "\n",
        "    plt.xlabel(\"Sentinel-2 Band\")\n",
        "    plt.ylabel(\"Reflectance\")\n",
        "    plt.title(\"Spectral Signature: Marine Debris vs. Other Classes\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(processed_dir, \"debris_vs_others_spectral.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return mean_signatures\n",
        "\n",
        "# Execute spectral analysis\n",
        "mean_spectral_signatures = extract_spectral_signatures()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYyc0a1IGqAe"
      },
      "outputs": [],
      "source": [
        "# Band Correlation Analysis\n",
        "\n",
        "def analyze_band_correlations(sample_size=20):\n",
        "    \"\"\"Analyze correlations between spectral bands\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Initialize band data collection\n",
        "    band_data = []\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Collecting band data\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find image files\n",
        "        image_files = [f for f in os.listdir(dir_path)\n",
        "                     if not f.endswith('_cl.tif') and not f.endswith('_conf.tif')\n",
        "                     and f.endswith('.tif')]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(dir_path, image_file)\n",
        "\n",
        "            with rasterio.open(image_path) as src:\n",
        "                img_data = src.read()\n",
        "                num_bands = img_data.shape[0]\n",
        "\n",
        "                # Flatten spatial dimensions, keep band dimension\n",
        "                img_flat = img_data.reshape(num_bands, -1)\n",
        "\n",
        "                # Sample pixels (to avoid memory issues)\n",
        "                num_pixels = img_flat.shape[1]\n",
        "                sample_indices = np.random.choice(num_pixels, min(1000, num_pixels), replace=False)\n",
        "                img_sample = img_flat[:, sample_indices]\n",
        "\n",
        "                # Transpose to have bands as columns\n",
        "                band_data.append(img_sample.T)\n",
        "\n",
        "    # Combine all samples\n",
        "    band_data_combined = np.vstack(band_data)\n",
        "\n",
        "    # Create DataFrame for correlation analysis\n",
        "    band_df = pd.DataFrame(band_data_combined,\n",
        "                          columns=[f\"Band_{i+1}\" for i in range(band_data_combined.shape[1])])\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = band_df.corr()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title(\"Band Correlation Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(processed_dir, \"band_correlation.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Identify highly correlated bands\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "                high_corr_pairs.append((\n",
        "                    corr_matrix.columns[i],\n",
        "                    corr_matrix.columns[j],\n",
        "                    corr_matrix.iloc[i, j]\n",
        "                ))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(\"Highly correlated band pairs (|r| > 0.8):\")\n",
        "        for band1, band2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "            print(f\"  {band1} and {band2}: r = {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"No highly correlated band pairs found (|r| > 0.8).\")\n",
        "\n",
        "    return corr_matrix\n",
        "\n",
        "# Execute band correlation analysis\n",
        "band_correlations = analyze_band_correlations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrvlwJfIG3X_"
      },
      "outputs": [],
      "source": [
        "# Spectral Indices Calculation\n",
        "\n",
        "def calculate_and_visualize_indices(sample_size=10):\n",
        "    \"\"\"Calculate and visualize spectral indices for marine debris detection\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    \"\"\" Define band indices for common Sentinel-2 bands\n",
        "        Assuming the spectral bands are in this order:\n",
        "        B2 (Blue), B3 (Green), B4 (Red), B5 (Red Edge 1), B6 (Red Edge 2),\n",
        "        B7 (Red Edge 3), B8 (NIR), B8A (NIR narrow), B11 (SWIR 1), B12 (SWIR 2)\"\"\"\n",
        "    blue_idx = 0    # B2\n",
        "    green_idx = 1   # B3\n",
        "    red_idx = 2     # B4\n",
        "    nir_idx = 6     # B8\n",
        "    swir1_idx = 8   # B11\n",
        "    swir2_idx = 9   # B12\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Processing spectral indices\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find image-mask pairs\n",
        "        image_files = [f for f in os.listdir(dir_path)\n",
        "                     if not f.endswith('_cl.tif') and not f.endswith('_conf.tif')\n",
        "                     and f.endswith('.tif')]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(dir_path, image_file)\n",
        "            mask_file = image_file.replace('.tif', '_cl.tif')\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            # Open image and mask\n",
        "            with rasterio.open(image_path) as img_src, rasterio.open(mask_path) as mask_src:\n",
        "                img_data = img_src.read()\n",
        "                mask_data = mask_src.read(1)\n",
        "\n",
        "                # Get metadata for saving indices\n",
        "                meta = img_src.meta.copy()\n",
        "                meta.update(count=1)\n",
        "\n",
        "                # Extract specific bands\n",
        "                try:\n",
        "                    blue = img_data[blue_idx]\n",
        "                    green = img_data[green_idx]\n",
        "                    red = img_data[red_idx]\n",
        "                    nir = img_data[nir_idx]\n",
        "                    swir1 = img_data[swir1_idx]\n",
        "                    swir2 = img_data[swir2_idx]\n",
        "                except IndexError:\n",
        "                    print(f\"  Warning: Not all required bands are available in {image_file}\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate spectral indices with epsilon to avoid division by zero\n",
        "                eps = 1e-8\n",
        "\n",
        "                # NDVI - Normalized Difference Vegetation Index\n",
        "                ndvi = (nir - red) / (nir + red + eps)\n",
        "\n",
        "                # NDWI - Normalized Difference Water Index (McFeeters)\n",
        "                ndwi = (green - nir) / (green + nir + eps)\n",
        "\n",
        "                # MNDWI - Modified Normalized Difference Water Index\n",
        "                mndwi = (green - swir1) / (green + swir1 + eps)\n",
        "\n",
        "                # AWEI - Automated Water Extraction Index\n",
        "                awei = 4 * (green - swir1) - (0.25 * nir + 2.75 * swir2)\n",
        "\n",
        "                # FDI - Floating Debris Index\n",
        "                fdi = nir - (red + (nir - red) * ((swir1 - red) / (nir - red + eps)))\n",
        "\n",
        "                # Calculate index for detecting plastic specifically\n",
        "                # PPI - Plastic Presence Index (Custom index)\n",
        "                ppi = (swir1 - nir) / (swir1 + nir + eps)\n",
        "\n",
        "                # Save directories for indices\n",
        "                indices_dir = os.path.join(processed_dir, \"indices\", s2_dir)\n",
        "                os.makedirs(indices_dir, exist_ok=True)\n",
        "\n",
        "                # Display RGB and indices for a sample image\n",
        "                if np.random.random() < 0.3:  # Only plot ~30% of processed images\n",
        "                    fig, axs = plt.subplots(2, 4, figsize=(18, 10))\n",
        "\n",
        "                    # RGB composite\n",
        "                    rgb = np.stack([red, green, blue], axis=0)\n",
        "                    rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                    # Normalize for display\n",
        "                    rgb_norm = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + eps)\n",
        "\n",
        "                    axs[0, 0].imshow(rgb_norm)\n",
        "                    axs[0, 0].set_title(\"RGB Composite\")\n",
        "\n",
        "                    # Create a mask for marine debris\n",
        "                    debris_mask = np.zeros_like(mask_data)\n",
        "                    debris_mask[mask_data == 1] = 1\n",
        "\n",
        "                    # Display mask\n",
        "                    axs[0, 1].imshow(debris_mask, cmap='binary_r')\n",
        "                    axs[0, 1].set_title(\"Marine Debris Mask\")\n",
        "\n",
        "                    # Display indices\n",
        "                    axs[0, 2].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "                    axs[0, 2].set_title(\"NDVI\")\n",
        "\n",
        "                    axs[0, 3].imshow(ndwi, cmap='Blues', vmin=-1, vmax=1)\n",
        "                    axs[0, 3].set_title(\"NDWI\")\n",
        "\n",
        "                    axs[1, 0].imshow(mndwi, cmap='Blues', vmin=-1, vmax=1)\n",
        "                    axs[1, 0].set_title(\"MNDWI\")\n",
        "\n",
        "                    axs[1, 1].imshow(awei, cmap='viridis')\n",
        "                    axs[1, 1].set_title(\"AWEI\")\n",
        "\n",
        "                    axs[1, 2].imshow(fdi, cmap='plasma')\n",
        "                    axs[1, 2].set_title(\"FDI\")\n",
        "\n",
        "                    axs[1, 3].imshow(ppi, cmap='RdBu', vmin=-1, vmax=1)\n",
        "                    axs[1, 3].set_title(\"PPI (Plastic)\")\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(indices_dir, f\"{image_file.replace('.tif', '_indices.png')}\"))\n",
        "                    plt.show()\n",
        "\n",
        "                    # Save indices as GeoTIFF\n",
        "                    indices_stack = np.stack([ndvi, ndwi, mndwi, awei, fdi, ppi])\n",
        "                    meta.update(count=6)\n",
        "\n",
        "                    with rasterio.open(\n",
        "                        os.path.join(indices_dir, f\"{image_file.replace('.tif', '_indices.tif')}\"),\n",
        "                        'w',\n",
        "                        **meta\n",
        "                    ) as dst:\n",
        "                        dst.write(indices_stack)\n",
        "\n",
        "    print(\"Spectral indices calculated and saved.\")\n",
        "\n",
        "# Execute spectral indices calculation\n",
        "calculate_and_visualize_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFwA7RXMP-iW"
      },
      "outputs": [],
      "source": [
        "# Confidence Level Analysis\n",
        "\n",
        "def analyze_confidence_levels(sample_size=30):\n",
        "    \"\"\"Analyze annotator confidence levels for different classes\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Initialize confidence data\n",
        "    class_confidence = {class_id: [] for class_id in range(1, 16)}\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Analyzing confidence levels\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find confidence mask files\n",
        "        conf_files = [f for f in os.listdir(dir_path) if f.endswith('_conf.tif')]\n",
        "\n",
        "        for conf_file in conf_files:\n",
        "            conf_path = os.path.join(dir_path, conf_file)\n",
        "            mask_file = conf_file.replace('_conf.tif', '_cl.tif')\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            # Open confidence and class masks\n",
        "            with rasterio.open(conf_path) as conf_src, rasterio.open(mask_path) as mask_src:\n",
        "                conf_data = conf_src.read(1)\n",
        "                mask_data = mask_src.read(1)\n",
        "\n",
        "                # For each class, extract confidence levels\n",
        "                for class_id in range(1, 16):\n",
        "                    class_pixels = np.where(mask_data == class_id)\n",
        "\n",
        "                    if len(class_pixels[0]) > 0:\n",
        "                        # Get confidence values for this class\n",
        "                        class_conf = conf_data[class_pixels]\n",
        "                        class_confidence[class_id].extend(class_conf.flatten())\n",
        "\n",
        "    # Calculate confidence statistics\n",
        "    confidence_stats = {}\n",
        "    for class_id, conf_values in class_confidence.items():\n",
        "        if conf_values:\n",
        "            confidence_stats[class_id] = {\n",
        "                'mean': np.mean(conf_values),\n",
        "                'median': np.median(conf_values),\n",
        "                'std': np.std(conf_values),\n",
        "                'min': np.min(conf_values),\n",
        "                'max': np.max(conf_values)\n",
        "            }\n",
        "\n",
        "    # Plot confidence distributions\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Only include classes with data\n",
        "    valid_classes = [class_id for class_id, values in class_confidence.items() if values]\n",
        "\n",
        "    # Create violin plot\n",
        "    violin_data = [class_confidence[class_id] for class_id in valid_classes]\n",
        "    violin_labels = [class_mapping[class_id] for class_id in valid_classes]\n",
        "\n",
        "    plt.violinplot(violin_data, showmedians=True)\n",
        "    plt.xticks(range(1, len(valid_classes) + 1), violin_labels, rotation=90)\n",
        "    plt.ylabel(\"Confidence Level\")\n",
        "    plt.title(\"Distribution of Annotator Confidence by Class\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(processed_dir, \"confidence_distribution.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(\"Annotator confidence statistics by class:\")\n",
        "    for class_id, stats in sorted(confidence_stats.items(), key=lambda x: x[1]['mean'], reverse=True):\n",
        "        print(f\"  {class_id}: {class_mapping[class_id]}\")\n",
        "        print(f\"    Mean: {stats['mean']:.2f}\")\n",
        "        print(f\"    Median: {stats['median']:.2f}\")\n",
        "        print(f\"    Std Dev: {stats['std']:.2f}\")\n",
        "        print(f\"    Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n",
        "\n",
        "    # Focus on marine debris confidence\n",
        "    if 1 in confidence_stats:\n",
        "        print(\"\\nMarine Debris confidence statistics:\")\n",
        "        stats = confidence_stats[1]\n",
        "        print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "        print(f\"  Median: {stats['median']:.2f}\")\n",
        "        print(f\"  Std Dev: {stats['std']:.2f}\")\n",
        "        print(f\"  Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n",
        "\n",
        "    return confidence_stats\n",
        "\n",
        "# Execute confidence level analysis\n",
        "confidence_statistics = analyze_confidence_levels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gyx5UktZQam"
      },
      "outputs": [],
      "source": [
        "# Temporal Analysis\n",
        "\n",
        "def analyze_temporal_patterns():\n",
        "    \"\"\"Analyze temporal patterns in debris occurrence\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Extract date information and debris presence\n",
        "    dates = []\n",
        "    debris_presence = []\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs, desc=\"Extracting temporal data\"):\n",
        "        # Extract date from directory name using a more flexible pattern\n",
        "        try:\n",
        "            # Try different pattern formats that might exist in your data\n",
        "            match = re.search(r'S2_(\\d{1,2}-\\d{1,2}-\\d{2})_(\\w+)', s2_dir)  # dd-mm-yy format\n",
        "            if not match:\n",
        "                match = re.search(r'S2_(\\d{1,2}-\\d{1,2}-\\d{4})_(\\w+)', s2_dir)  # dd-mm-yyyy format\n",
        "            if not match:\n",
        "                match = re.search(r'S2_(\\d{8})_(\\w+)', s2_dir)  # yyyymmdd format (original)\n",
        "\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "\n",
        "                # Handle different date formats\n",
        "                try:\n",
        "                    date = datetime.strptime(date_str, '%d-%m-%y')\n",
        "                except ValueError:\n",
        "                    try:\n",
        "                        date = datetime.strptime(date_str, '%d-%m-%Y')\n",
        "                    except ValueError:\n",
        "                        date = datetime.strptime(date_str, '%Y%m%d')\n",
        "\n",
        "                dates.append(date)\n",
        "\n",
        "            # Check for debris presence in any image within the directory\n",
        "            dir_path = os.path.join(patches_dir, s2_dir)\n",
        "            mask_files = [f for f in os.listdir(dir_path) if f.endswith('_cl.tif')]\n",
        "\n",
        "            has_debris = False\n",
        "            for mask_file in mask_files:\n",
        "                mask_path = os.path.join(dir_path, mask_file)\n",
        "                with rasterio.open(mask_path) as src:\n",
        "                    mask_data = src.read(1)\n",
        "                    if np.any(mask_data == 1):  # Check for marine debris (class 1)\n",
        "                        has_debris = True\n",
        "                        break\n",
        "\n",
        "            debris_presence.append(has_debris)\n",
        "\n",
        "        except AttributeError:\n",
        "            print(f\"Warning: Could not extract date from directory name: {s2_dir}\")\n",
        "\n",
        "    # Create DataFrame for analysis\n",
        "    temporal_df = pd.DataFrame({'date': dates, 'debris_present': debris_presence})\n",
        "\n",
        "    # Group by date and count debris occurrences\n",
        "    debris_counts = temporal_df.groupby('date')['debris_present'].sum().reset_index()\n",
        "\n",
        "    # Plot temporal patterns\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(debris_counts['date'], debris_counts['debris_present'], marker='o')\n",
        "    plt.title('Temporal Pattern of Marine Debris Detection')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Locations with Debris')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(processed_dir, \"temporal_patterns.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return debris_counts\n",
        "\n",
        "# Execute temporal analysis\n",
        "temporal_patterns = analyze_temporal_patterns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgVDCbltZHJd"
      },
      "outputs": [],
      "source": [
        "# Object Size and Shape Analysis\n",
        "\n",
        "def analyze_object_properties(sample_size=50):\n",
        "    \"\"\"Analyze the size and shape of debris objects\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Initialize lists to store object properties\n",
        "    areas = []\n",
        "    perimeters = []\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Analyzing object properties\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "        mask_files = [f for f in os.listdir(dir_path) if f.endswith('_cl.tif')]\n",
        "\n",
        "        for mask_file in mask_files:\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "            with rasterio.open(mask_path) as src:\n",
        "                mask_data = src.read(1)\n",
        "\n",
        "                # Find debris objects (class 1)\n",
        "                debris_objects = mask_data == 1\n",
        "\n",
        "                # Label connected components\n",
        "                from skimage.measure import label, regionprops\n",
        "                labeled_objects = label(debris_objects)\n",
        "\n",
        "                # Calculate properties for each object\n",
        "                for region in regionprops(labeled_objects):\n",
        "                    areas.append(region.area)\n",
        "                    perimeters.append(region.perimeter)\n",
        "\n",
        "    # Plot distributions\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Area distribution\n",
        "    axs[0].hist(areas, bins=50, color='green', edgecolor='black')\n",
        "    axs[0].set_title('Distribution of Debris Object Areas')\n",
        "    axs[0].set_xlabel('Area (pixels)')\n",
        "    axs[0].set_ylabel('Frequency')\n",
        "\n",
        "    # Perimeter distribution\n",
        "    axs[1].hist(perimeters, bins=50, color='blue', edgecolor='black')\n",
        "    axs[1].set_title('Distribution of Debris Object Perimeters')\n",
        "    axs[1].set_xlabel('Perimeter (pixels)')\n",
        "    axs[1].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(processed_dir, \"object_properties.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return {'areas': areas, 'perimeters': perimeters}\n",
        "\n",
        "# Execute object properties analysis\n",
        "object_properties = analyze_object_properties()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfU2mXPMapcv"
      },
      "outputs": [],
      "source": [
        "# Data Visualization with Plastic Debris Highlighting\n",
        "\n",
        "def visualize_data_with_plastic_highlight(sample_size=10):\n",
        "    \"\"\"Visualize sample data and highlight plastic debris\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Visualizing data\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find image-mask pairs\n",
        "        image_files = [f for f in os.listdir(dir_path)\n",
        "                       if not f.endswith('_cl.tif') and not f.endswith('_conf.tif')\n",
        "                       and f.endswith('.tif')]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(dir_path, image_file)\n",
        "            mask_file = image_file.replace('.tif', '_cl.tif')\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            # Open image and mask\n",
        "            with rasterio.open(image_path) as img_src, rasterio.open(mask_path) as mask_src:\n",
        "                img_data = img_src.read()\n",
        "                mask_data = mask_src.read(1)\n",
        "\n",
        "                # Create RGB composite for visualization\n",
        "                rgb = np.stack([img_data[2], img_data[1], img_data[0]], axis=0)  # Assuming R, G, B order\n",
        "                rgb = np.transpose(rgb, (1, 2, 0))\n",
        "                rgb_norm = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb))  # Normalize for display\n",
        "\n",
        "                # Create a mask for plastic debris (class 1)\n",
        "                plastic_mask = np.zeros_like(mask_data)\n",
        "                plastic_mask[mask_data == 1] = 1\n",
        "\n",
        "                # Highlight plastic debris on the image\n",
        "                highlighted_image = rgb_norm.copy()\n",
        "                highlighted_image[plastic_mask == 1] = [1, 0, 0]  # Highlight in red\n",
        "\n",
        "                # Display the image and highlighted image\n",
        "                fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
        "                axs[0].imshow(rgb_norm)\n",
        "                axs[0].set_title(\"Original Image\")\n",
        "                axs[1].imshow(highlighted_image)\n",
        "                axs[1].set_title(\"Plastic Debris Highlighted\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "visualize_data_with_plastic_highlight(sample_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB_eQU6PLuNd"
      },
      "outputs": [],
      "source": [
        "# Custom Indices for Plastic Debris\n",
        "\n",
        "def develop_plastic_indices(sample_size=20):\n",
        "    \"\"\"Develop and evaluate custom indices specifically for plastic debris detection\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "\n",
        "    # Get list of all S2 date-tile directories\n",
        "    s2_dirs = [d for d in os.listdir(patches_dir) if os.path.isdir(os.path.join(patches_dir, d))]\n",
        "\n",
        "    # Randomly sample directories for analysis\n",
        "    if len(s2_dirs) > sample_size:\n",
        "        import random\n",
        "        s2_dirs_sample = random.sample(s2_dirs, sample_size)\n",
        "    else:\n",
        "        s2_dirs_sample = s2_dirs\n",
        "\n",
        "    # Define band indices for common Sentinel-2 bands\n",
        "    blue_idx = 0    # B2\n",
        "    green_idx = 1   # B3\n",
        "    red_idx = 2     # B4\n",
        "    rededge1_idx = 3  # B5\n",
        "    rededge2_idx = 4  # B6\n",
        "    rededge3_idx = 5  # B7\n",
        "    nir_idx = 6     # B8\n",
        "    nir2_idx = 7    # B8A\n",
        "    swir1_idx = 8   # B11\n",
        "    swir2_idx = 9   # B12\n",
        "\n",
        "    # Initialize statistics for each index\n",
        "    index_stats = {\n",
        "        'PFI': {'debris': [], 'non_debris': []},  # Plastic Float Index\n",
        "        'NDPI': {'debris': [], 'non_debris': []}, # Normalized Difference Plastic Index\n",
        "        'PDRI': {'debris': [], 'non_debris': []}, # Plastic Debris Ratio Index\n",
        "        'PFDI': {'debris': [], 'non_debris': []}  # Plastic Floating Debris Index\n",
        "    }\n",
        "\n",
        "    for s2_dir in tqdm(s2_dirs_sample, desc=\"Calculating plastic indices\"):\n",
        "        dir_path = os.path.join(patches_dir, s2_dir)\n",
        "\n",
        "        # Find image-mask pairs\n",
        "        image_files = [f for f in os.listdir(dir_path)\n",
        "                     if not f.endswith('_cl.tif') and not f.endswith('_conf.tif')\n",
        "                     and f.endswith('.tif')]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(dir_path, image_file)\n",
        "            mask_file = image_file.replace('.tif', '_cl.tif')\n",
        "            mask_path = os.path.join(dir_path, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            # Open image and mask\n",
        "            with rasterio.open(image_path) as img_src, rasterio.open(mask_path) as mask_src:\n",
        "                img_data = img_src.read()\n",
        "                mask_data = mask_src.read(1)\n",
        "\n",
        "                # Extract debris mask\n",
        "                debris_mask = mask_data == 1\n",
        "                non_debris_mask = (mask_data > 1) & (mask_data < 16)  # All other valid classes\n",
        "\n",
        "                # Skip if no debris or non-debris pixels\n",
        "                if not np.any(debris_mask) or not np.any(non_debris_mask):\n",
        "                    continue\n",
        "\n",
        "                # Extract bands\n",
        "                try:\n",
        "                    blue = img_data[blue_idx]\n",
        "                    green = img_data[green_idx]\n",
        "                    red = img_data[red_idx]\n",
        "                    rededge1 = img_data[rededge1_idx]\n",
        "                    nir = img_data[nir_idx]\n",
        "                    swir1 = img_data[swir1_idx]\n",
        "                    swir2 = img_data[swir2_idx]\n",
        "                except IndexError:\n",
        "                    continue\n",
        "\n",
        "                # Add small epsilon to avoid division by zero\n",
        "                eps = 1e-8\n",
        "\n",
        "                # Calculate custom plastic indices\n",
        "\n",
        "                # PFI - Plastic Float Index (highlighting floating materials with plastic spectral characteristics)\n",
        "                pfi = (swir1 - nir) / (swir1 + nir + eps) * (rededge1 / (blue + eps))\n",
        "\n",
        "                # NDPI - Normalized Difference Plastic Index (targeting specific plastic reflectance features)\n",
        "                ndpi = (swir1 - red) / (swir1 + red + eps)\n",
        "\n",
        "                # PDRI - Plastic Debris Ratio Index (ratio-based approach using key bands)\n",
        "                pdri = (swir1 / (nir + eps)) * (blue / (green + eps))\n",
        "\n",
        "                # PFDI - Plastic Floating Debris Index (complex index targeting floating plastic)\n",
        "                pfdi = (swir1 - blue) / (swir1 + blue + eps) * (nir / (red + eps)) - (swir2 / (swir1 + eps))\n",
        "\n",
        "                # Collect statistics for each index\n",
        "                indices = {'PFI': pfi, 'NDPI': ndpi, 'PDRI': pdri, 'PFDI': pfdi}\n",
        "\n",
        "                for index_name, index_values in indices.items():\n",
        "                    # Sample values (to avoid memory issues with large images)\n",
        "                    debris_pixels = index_values[debris_mask]\n",
        "                    if len(debris_pixels) > 1000:\n",
        "                        debris_pixels = np.random.choice(debris_pixels, 1000, replace=False)\n",
        "\n",
        "                    non_debris_pixels = index_values[non_debris_mask]\n",
        "                    if len(non_debris_pixels) > 1000:\n",
        "                        non_debris_pixels = np.random.choice(non_debris_pixels, 1000, replace=False)\n",
        "\n",
        "                    index_stats[index_name]['debris'].extend(debris_pixels)\n",
        "                    index_stats[index_name]['non_debris'].extend(non_debris_pixels)\n",
        "\n",
        "    # Analyze separation capability of each index\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    for i, (index_name, values) in enumerate(index_stats.items()):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        debris_values = np.array(values['debris'])\n",
        "        non_debris_values = np.array(values['non_debris'])\n",
        "\n",
        "        # Remove extreme outliers\n",
        "        debris_q1, debris_q3 = np.percentile(debris_values, [1, 99])\n",
        "        non_debris_q1, non_debris_q3 = np.percentile(non_debris_values, [1, 99])\n",
        "\n",
        "        debris_values = debris_values[(debris_values >= debris_q1) & (debris_values <= debris_q3)]\n",
        "        non_debris_values = non_debris_values[(non_debris_values >= non_debris_q1) & (non_debris_values <= non_debris_q3)]\n",
        "\n",
        "        # Plot histograms\n",
        "        plt.hist(debris_values, bins=50, alpha=0.7, label='Marine Debris', color='red')\n",
        "        plt.hist(non_debris_values, bins=50, alpha=0.7, label='Non-Debris', color='blue')\n",
        "\n",
        "        plt.title(f\"{index_name} Distribution\")\n",
        "        plt.xlabel(\"Index Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.legend()\n",
        "\n",
        "        # Calculate and display separation metrics\n",
        "        d_mean = np.mean(debris_values)\n",
        "        nd_mean = np.mean(non_debris_values)\n",
        "        d_std = np.std(debris_values)\n",
        "        nd_std = np.std(non_debris_values)\n",
        "\n",
        "        # Fisher's Discriminant Ratio (higher is better)\n",
        "        fdr = ((d_mean - nd_mean) ** 2) / (d_std ** 2 + nd_std ** 2 + eps)\n",
        "\n",
        "        plt.text(0.05, 0.95, f\"Mean (Debris): {d_mean:.3f}\\nMean (Non-Debris): {nd_mean:.3f}\\nFDR: {fdr:.3f}\",\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top',\n",
        "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(processed_dir, \"plastic_indices_evaluation.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Return the index with the highest FDR as the best one\n",
        "    best_index = max(index_stats.keys(), key=lambda idx:\n",
        "                   ((np.mean(index_stats[idx]['debris']) - np.mean(index_stats[idx]['non_debris'])) ** 2) /\n",
        "                   (np.std(index_stats[idx]['debris']) ** 2 + np.std(index_stats[idx]['non_debris']) ** 2 + eps))\n",
        "\n",
        "    print(f\"Best plastic debris detection index: {best_index}\")\n",
        "    return index_stats, best_index\n",
        "\n",
        "# Execute plastic indices development\n",
        "plastic_indices_stats, best_plastic_index = develop_plastic_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAAMB573ffOm"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing for UNeXt Model with restlite encoder Training\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_data_for_training():\n",
        "    \"\"\"Preprocess MARIDA data for training the Attention U-Net model\"\"\"\n",
        "    patches_dir = os.path.join(base_dir, \"patches\")\n",
        "    splits_dir = os.path.join(base_dir, \"splits\")\n",
        "\n",
        "    # Create output directory for preprocessed data\n",
        "    preprocessed_dir = os.path.join(processed_dir, \"preprocessed\")\n",
        "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "\n",
        "    # Load splits\n",
        "    train_paths = []\n",
        "    val_paths = []\n",
        "    test_paths = []\n",
        "\n",
        "    # Check if predefined splits exist\n",
        "    if os.path.exists(os.path.join(splits_dir, \"train_1.txt\")):\n",
        "        print(\"Using predefined dataset splits...\")\n",
        "        with open(os.path.join(splits_dir, \"train_1.txt\"), 'r') as f:\n",
        "            train_paths = [line.strip() for line in f.readlines()]\n",
        "        with open(os.path.join(splits_dir, \"val_1.txt\"), 'r') as f:\n",
        "            val_paths = [line.strip() for line in f.readlines()]\n",
        "        with open(os.path.join(splits_dir, \"test_1.txt\"), 'r') as f:\n",
        "            test_paths = [line.strip() for line in f.readlines()]\n",
        "    else:\n",
        "        print(\"Creating new dataset splits...\")\n",
        "        # Get all image paths\n",
        "        all_image_paths = []\n",
        "        for s2_dir in os.listdir(patches_dir):\n",
        "            dir_path = os.path.join(patches_dir, s2_dir)\n",
        "            if os.path.isdir(dir_path):\n",
        "                image_files = [f for f in os.listdir(dir_path)\n",
        "                            if not f.endswith('_cl.tif') and not f.endswith('_conf.tif')\n",
        "                            and f.endswith('.tif')]\n",
        "                for img_file in image_files:\n",
        "                    # Check if mask exists\n",
        "                    mask_file = img_file.replace('.tif', '_cl.tif')\n",
        "                    mask_path = os.path.join(dir_path, mask_file)\n",
        "                    if os.path.exists(mask_path):\n",
        "                        all_image_paths.append(os.path.join(s2_dir, img_file))\n",
        "\n",
        "        # Split data\n",
        "        train_val_paths, test_paths = train_test_split(all_image_paths, test_size=0.2, random_state=42)\n",
        "        train_paths, val_paths = train_test_split(train_val_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Save splits for future use\n",
        "        os.makedirs(splits_dir, exist_ok=True)\n",
        "        with open(os.path.join(splits_dir, \"train_custom.txt\"), 'w') as f:\n",
        "            f.write('\\n'.join(train_paths))\n",
        "        with open(os.path.join(splits_dir, \"val_custom.txt\"), 'w') as f:\n",
        "            f.write('\\n'.join(val_paths))\n",
        "        with open(os.path.join(splits_dir, \"test_custom.txt\"), 'w') as f:\n",
        "            f.write('\\n'.join(test_paths))\n",
        "\n",
        "    print(f\"Dataset splits: {len(train_paths)} training, {len(val_paths)} validation, {len(test_paths)} test samples\")\n",
        "\n",
        "    # Create metadata files with class frequencies\n",
        "    analyze_split_class_distribution(train_paths, \"train\", patches_dir, preprocessed_dir)\n",
        "    analyze_split_class_distribution(val_paths, \"val\", patches_dir, preprocessed_dir)\n",
        "    analyze_split_class_distribution(test_paths, \"test\", patches_dir, preprocessed_dir)\n",
        "\n",
        "    # Save paths for DataLoader\n",
        "    split_paths = {\n",
        "        'train': train_paths,\n",
        "        'val': val_paths,\n",
        "        'test': test_paths\n",
        "    }\n",
        "\n",
        "    # Save paths for later use\n",
        "    with open(os.path.join(preprocessed_dir, \"split_paths.pkl\"), 'wb') as f:\n",
        "        import pickle\n",
        "        pickle.dump(split_paths, f)\n",
        "\n",
        "    print(\"Data preprocessing completed and paths saved.\")\n",
        "    return split_paths\n",
        "\n",
        "def analyze_split_class_distribution(paths, split_name, patches_dir, output_dir):\n",
        "    \"\"\"Analyze class distribution in a dataset split\"\"\"\n",
        "    class_counts = {i: 0 for i in range(1, 16)}\n",
        "    total_pixels = 0\n",
        "    has_debris_count = 0\n",
        "\n",
        "    for path in tqdm(paths, desc=f\"Analyzing {split_name} split\"):\n",
        "        dir_name = os.path.dirname(path)\n",
        "        img_name = os.path.basename(path)\n",
        "        mask_name = img_name.replace('.tif', '_cl.tif')\n",
        "        mask_path = os.path.join(patches_dir, dir_name, mask_name)\n",
        "\n",
        "        if os.path.exists(mask_path):\n",
        "            with rasterio.open(mask_path) as src:\n",
        "                mask_data = src.read(1)\n",
        "\n",
        "                # Count class pixels\n",
        "                has_debris = False\n",
        "                for class_id in range(1, 16):\n",
        "                    count = np.sum(mask_data == class_id)\n",
        "                    class_counts[class_id] += count\n",
        "\n",
        "                    if class_id == 1 and count > 0:\n",
        "                        has_debris = True\n",
        "\n",
        "                if has_debris:\n",
        "                    has_debris_count += 1\n",
        "\n",
        "                total_pixels += mask_data.size\n",
        "\n",
        "    # Calculate percentages\n",
        "    class_percentages = {class_id: (count / total_pixels * 100)\n",
        "                        for class_id, count in class_counts.items()}\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(output_dir, f\"{split_name}_class_distribution.txt\"), 'w') as f:\n",
        "        f.write(f\"Class distribution in {split_name} split:\\n\")\n",
        "        f.write(f\"Total images: {len(paths)}\\n\")\n",
        "        f.write(f\"Images with debris: {has_debris_count} ({has_debris_count/len(paths)*100:.2f}%)\\n\\n\")\n",
        "\n",
        "        for class_id, percentage in sorted(class_percentages.items(), key=lambda x: x[1], reverse=True):\n",
        "            f.write(f\"{class_id}: {class_mapping[class_id]} - {percentage:.2f}%\\n\")\n",
        "\n",
        "    # Calculate class weights (  inverse frequency)\n",
        "    class_weights = {class_id: (total_pixels / (count + 1))\n",
        "                   for class_id, count in class_counts.items()}\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    sum_weights = sum(class_weights.values())\n",
        "    normalized_weights = {class_id: weight / sum_weights for class_id, weight in class_weights.items()}\n",
        "\n",
        "    # Save weights for training\n",
        "    np.save(os.path.join(output_dir, f\"{split_name}_class_weights.npy\"),\n",
        "           np.array([normalized_weights[i] for i in range(1, 16)]))\n",
        "\n",
        "    return class_percentages\n",
        "\n",
        "# MARIDA Dataset class\n",
        "class MARIDADataset(Dataset):\n",
        "    def __init__(self, paths, patches_dir, transform=None, use_multispectral=True):\n",
        "        self.paths = paths\n",
        "        self.patches_dir = patches_dir\n",
        "        self.transform = transform\n",
        "        self.use_multispectral = use_multispectral\n",
        "        self.rgb_bands = [2, 1, 0]  # B4, B3, B2 (0 indexed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_path = self.paths[idx]\n",
        "            dir_name = os.path.dirname(img_path)\n",
        "            img_name = os.path.basename(img_path)\n",
        "            full_img_path = os.path.join(self.patches_dir, dir_name, img_name)\n",
        "            mask_name = img_name.replace('.tif', '_cl.tif')\n",
        "            full_mask_path = os.path.join(self.patches_dir, dir_name, mask_name)\n",
        "\n",
        "            with rasterio.open(full_img_path) as src:\n",
        "                image = src.read()\n",
        "\n",
        "            with rasterio.open(full_mask_path) as src:\n",
        "                mask = src.read(1)\n",
        "\n",
        "            # Ensure mask is binary (0 or 1)\n",
        "            binary_mask = (mask == 1).astype(np.float32)\n",
        "\n",
        "            # Rest of the preprocessing...\n",
        "            if not self.use_multispectral:\n",
        "                image = image[self.rgb_bands]\n",
        "\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "            # Calculating band ratios and indices that highlight plastics\n",
        "            if self.use_multispectral:\n",
        "                nir_band = 3\n",
        "                red_band = 2\n",
        "                ndvi = (image[nir_band] - image[red_band]) / (image[nir_band] + image[red_band] + 1e-7)\n",
        "                image = np.vstack([image, ndvi.reshape(1, image.shape[1], image.shape[2])])\n",
        "\n",
        "            # Normalizing each band\n",
        "            for i in range(image.shape[0]):\n",
        "                band_min, band_max = np.nanmin(image[i]), np.nanmax(image[i])\n",
        "                if np.isnan(band_min) or np.isnan(band_max):\n",
        "                    image[i] = np.nan_to_num(image[i], nan=0.0)\n",
        "                    band_min, band_max = 0, 1\n",
        "\n",
        "                if band_max > band_min:\n",
        "                    image[i] = (image[i] - band_min) / (band_max - band_min)\n",
        "                else:\n",
        "                    image[i] = np.zeros_like(image[i])\n",
        "\n",
        "            image = np.nan_to_num(image, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "\n",
        "            if self.transform:\n",
        "                transformed = self.transform(image=np.transpose(image, (1, 2, 0)), mask=binary_mask)\n",
        "                image = transformed['image']\n",
        "                binary_mask = transformed['mask']\n",
        "\n",
        "                if not torch.is_tensor(image):\n",
        "                    image = torch.from_numpy(image.transpose(2, 0, 1))\n",
        "                if not torch.is_tensor(binary_mask):\n",
        "                    binary_mask = torch.from_numpy(binary_mask).unsqueeze(0)\n",
        "            else:\n",
        "                image = torch.from_numpy(image)\n",
        "                binary_mask = torch.from_numpy(binary_mask).unsqueeze(0)\n",
        "\n",
        "            return image, binary_mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            if self.use_multispectral:\n",
        "                dummy_img = torch.zeros((12, 256, 256), dtype=torch.float32)\n",
        "            else:\n",
        "                dummy_img = torch.zeros((3, 256, 256), dtype=torch.float32)\n",
        "            dummy_mask = torch.zeros((1, 256, 256), dtype=torch.float32)\n",
        "            return dummy_img, dummy_mask\n",
        "\n",
        "# Defining training transform with augmentation\n",
        "train_transform = A.Compose([\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ElasticTransform(p=0.3),  # Add elastic transform\n",
        "    A.GridDistortion(p=0.2),    # Add grid distortion\n",
        "    A.OpticalDistortion(p=0.2), # Add optical distortion\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    A.OneOf([  # Add one of these channel-specific augmentations\n",
        "        A.RandomGamma(p=1.0),\n",
        "        A.ChannelShuffle(p=1.0),  # its good for multispectral data\n",
        "        A.ChannelDropout(p=1.0),  # it randomly drop channels\n",
        "    ], p=0.3),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Define validation transform (no augmentation, justnormalization)\n",
        "val_transform = A.Compose([\n",
        "    ToTensorV2()  # itsonly convert to tensor, no augmentation\n",
        "])\n",
        "\n",
        "def prepare_data_loaders(split_paths, patches_dir, batch_size=8, num_workers=4, use_multispectral=True):\n",
        "    # Calculate sample weights - more weight for samples with debris\n",
        "    sample_weights = []\n",
        "    for path in split_paths['train']:\n",
        "        dir_name = os.path.dirname(path)\n",
        "        img_name = os.path.basename(path)\n",
        "        mask_name = img_name.replace('.tif', '_cl.tif')\n",
        "        mask_path = os.path.join(patches_dir, dir_name, mask_name)\n",
        "\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask = src.read(1)\n",
        "            has_debris = (mask == 1).any()\n",
        "            weight = 3.0 if has_debris else 1.0  # Higher weight for debris samples\n",
        "            sample_weights.append(weight)\n",
        "\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(split_paths['train']),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MARIDADataset(\n",
        "        split_paths['train'],\n",
        "        patches_dir,\n",
        "        transform=train_transform,  # Useing train_transform\n",
        "        use_multispectral=use_multispectral\n",
        "    )\n",
        "\n",
        "    val_dataset = MARIDADataset(\n",
        "        split_paths['val'],\n",
        "        patches_dir,\n",
        "        transform=val_transform,  # Useing val_transform\n",
        "        use_multispectral=use_multispectral\n",
        "    )\n",
        "\n",
        "    test_dataset = MARIDADataset(\n",
        "        split_paths['test'],\n",
        "        patches_dir,\n",
        "        transform=val_transform,  # Useing val_transform for test as well\n",
        "        use_multispectral=use_multispectral\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Testing samples: {len(test_dataset)}\")\n",
        "    sample_img, _ = train_dataset[0]\n",
        "    print(f\"Input image shape: {sample_img.shape}\")\n",
        "    print(f\"Using {'multispectral' if use_multispectral else 'RGB'} data\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Execute data preprocessing\n",
        "split_paths = preprocess_data_for_training()\n",
        "train_loader, val_loader, test_loader = prepare_data_loaders(\n",
        "    split_paths,\n",
        "    os.path.join(base_dir, \"patches\"),\n",
        "    batch_size=8,\n",
        "    use_multispectral=True  # Set to True for multispectral approach\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbBzoHz-ffLo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "# ResT-Lite Component Implementations\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.sr_ratio = sr_ratio\n",
        "        if sr_ratio > 1:\n",
        "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        if self.sr_ratio > 1:\n",
        "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
        "            x_ = self.norm(x_)\n",
        "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        else:\n",
        "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = (img_size, img_size) if isinstance(img_size, int) else img_size\n",
        "        patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n",
        "        H, W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.H, self.W = H, W\n",
        "        self.num_patches = H * W\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "        H, W = H // self.patch_size[0], W // self.patch_size[1]\n",
        "        return x, (H, W)\n",
        "\n",
        "class ResTLite(nn.Module):\n",
        "    \"\"\"ResT-Lite backbone implementation for UNeXt model\"\"\"\n",
        "    def __init__(self, in_chans=3, depths=[2, 2, 2, 2], embed_dims=[64, 128, 256, 512],\n",
        "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., sr_ratios=[8, 4, 2, 1],\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
        "        super().__init__()\n",
        "        self.depths = depths\n",
        "        self.num_stages = len(depths)\n",
        "\n",
        "        # Patch embeddings\n",
        "        self.patch_embed1 = PatchEmbed(img_size=256, patch_size=4, in_chans=in_chans, embed_dim=embed_dims[0])\n",
        "        self.patch_embed2 = PatchEmbed(img_size=64, patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
        "        self.patch_embed3 = PatchEmbed(img_size=32, patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
        "        self.patch_embed4 = PatchEmbed(img_size=16, patch_size=2, in_chans=embed_dims[2], embed_dim=embed_dims[3])\n",
        "\n",
        "        # Transformer blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "\n",
        "        self.block1 = nn.ModuleList([\n",
        "            Block(dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias,\n",
        "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur+i], sr_ratio=sr_ratios[0],\n",
        "                  norm_layer=norm_layer) for i in range(depths[0])\n",
        "        ])\n",
        "        self.norm1 = norm_layer(embed_dims[0])\n",
        "        cur += depths[0]\n",
        "\n",
        "        self.block2 = nn.ModuleList([\n",
        "            Block(dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias,\n",
        "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur+i], sr_ratio=sr_ratios[1],\n",
        "                  norm_layer=norm_layer) for i in range(depths[1])\n",
        "        ])\n",
        "        self.norm2 = norm_layer(embed_dims[1])\n",
        "        cur += depths[1]\n",
        "\n",
        "        self.block3 = nn.ModuleList([\n",
        "            Block(dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias,\n",
        "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur+i], sr_ratio=sr_ratios[2],\n",
        "                  norm_layer=norm_layer) for i in range(depths[2])\n",
        "        ])\n",
        "        self.norm3 = norm_layer(embed_dims[2])\n",
        "        cur += depths[2]\n",
        "\n",
        "        self.block4 = nn.ModuleList([\n",
        "            Block(dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias,\n",
        "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur+i], sr_ratio=sr_ratios[3],\n",
        "                  norm_layer=norm_layer) for i in range(depths[3])\n",
        "        ])\n",
        "        self.norm4 = norm_layer(embed_dims[3])\n",
        "\n",
        "        # Weights initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        features = []\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Stage 1\n",
        "        x, (H, W) = self.patch_embed1(x)\n",
        "        for blk in self.block1:\n",
        "            x = blk(x, H, W)\n",
        "        x = self.norm1(x)\n",
        "        x1 = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        features.append(x1)\n",
        "\n",
        "        # Stage 2\n",
        "        x, (H, W) = self.patch_embed2(x1)\n",
        "        for blk in self.block2:\n",
        "            x = blk(x, H, W)\n",
        "        x = self.norm2(x)\n",
        "        x2 = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        features.append(x2)\n",
        "\n",
        "        # Stage 3\n",
        "        x, (H, W) = self.patch_embed3(x2)\n",
        "        for blk in self.block3:\n",
        "            x = blk(x, H, W)\n",
        "        x = self.norm3(x)\n",
        "        x3 = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        features.append(x3)\n",
        "\n",
        "        # Stage 4\n",
        "        x, (H, W) = self.patch_embed4(x3)\n",
        "        for blk in self.block4:\n",
        "            x = blk(x, H, W)\n",
        "        x = self.norm4(x)\n",
        "        x4 = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "        features.append(x4)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_features(x)\n",
        "\n",
        "# UNeXt model components - Fixed ConvNextBlock\n",
        "class ConvNextBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim_out)\n",
        "\n",
        "        # Add projection layer if dimensions change\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim_out)),\n",
        "                                requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_x = x\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "\n",
        "        if self.gamma is not None:\n",
        "            x = x * self.gamma\n",
        "\n",
        "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        # Apply projection to input if dimensions differ\n",
        "        input_x = self.proj(input_x)\n",
        "\n",
        "        # Apply drop path and add residual\n",
        "        x = input_x + self.drop_path(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Fixed Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # Calculate combined channels after concatenation\n",
        "        combined_channels = out_channels + (skip_channels if skip_channels > 0 else 0)\n",
        "\n",
        "        self.conv = ConvNextBlock(combined_channels, out_channels, drop_path=drop_path)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.up(x)\n",
        "\n",
        "        if skip is not None:\n",
        "            # this helps to handle different spatial dimensions\n",
        "            if x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n",
        "                x = F.interpolate(x, size=(skip.shape[2], skip.shape[3]),\n",
        "                                 mode='bilinear', align_corners=True)\n",
        "\n",
        "            # Concatenate skip connection\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNeXt(nn.Module):\n",
        "    def __init__(self, in_channels=12, out_channels=1,\n",
        "                backbone_dims=[64, 128, 256, 512], drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        # ResT-Lite backbone\n",
        "        self.backbone = ResTLite(in_chans=in_channels, embed_dims=backbone_dims)\n",
        "\n",
        "        # Decoder path with proper channel alignment\n",
        "        drop_path_values = torch.linspace(0, drop_path_rate, 4).tolist()\n",
        "\n",
        "        # Initialize decoders with correct channel sizes\n",
        "        self.decoder1 = Decoder(backbone_dims[3], backbone_dims[2], backbone_dims[2], drop_path_values[0])\n",
        "        self.decoder2 = Decoder(backbone_dims[2], backbone_dims[1], backbone_dims[1], drop_path_values[1])\n",
        "        self.decoder3 = Decoder(backbone_dims[1], backbone_dims[0], backbone_dims[0], drop_path_values[2])\n",
        "        self.decoder4 = Decoder(backbone_dims[0], 0, backbone_dims[0]//2, drop_path_values[3])\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(backbone_dims[0]//2, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Add an upsampling layer to ensure output resolution matches input\n",
        "        self.final_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original spatial dimensions\n",
        "        _, _, H, W = x.shape\n",
        "\n",
        "        # Get features from backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Apply decoders\n",
        "        x = self.decoder1(features[3], features[2])\n",
        "        x = self.decoder2(x, features[1])\n",
        "        x = self.decoder3(x, features[0])\n",
        "        x = self.decoder4(x)\n",
        "\n",
        "        # Apply final convolution\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # Ensure output resolution matches input resolution\n",
        "        if x.shape[2] != H or x.shape[3] != W:\n",
        "            x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n",
        "\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Loss Functions\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, pos_weight=10.0):\n",
        "        super().__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        # Binary cross entropy with weights, maintaining batch dimensions\n",
        "        pred_sigmoid = torch.clamp(pred, min=1e-7, max=1.0-1e-7)\n",
        "        loss = -self.pos_weight * target * torch.log(pred_sigmoid) - (1 - target) * torch.log(1 - pred_sigmoid)\n",
        "        return loss.mean()\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.9, gamma=3.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        pred = torch.clamp(pred, min=self.eps, max=1.0 - self.eps)\n",
        "        bce_loss = -target * torch.log(pred) - (1 - target) * torch.log(1 - pred)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "        alpha_weight = target * self.alpha + (1 - target) * (1 - self.alpha)\n",
        "        focal_weight = alpha_weight * focal_weight\n",
        "        return (focal_weight * bce_loss).mean()\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-5):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Make sure target has same shape as predictions\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        # Flatten predictions and targets properly\n",
        "        batch_size = pred.size(0)\n",
        "        pred_flat = pred.view(batch_size, -1)\n",
        "        target_flat = target.view(batch_size, -1)\n",
        "\n",
        "        # Calculate Dice score per batch and then mean\n",
        "        intersection = (pred_flat * target_flat).sum(dim=1)\n",
        "        union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
        "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
        "\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, beta=0.3, smooth=1e-5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Make sure target has same shape as predictions\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        # Flatten predictions and targets properly\n",
        "        batch_size = pred.size(0)\n",
        "        pred_flat = pred.view(batch_size, -1)\n",
        "        target_flat = target.view(batch_size, -1)\n",
        "\n",
        "        # True Positive, False Negative, False Positive\n",
        "        TP = (pred_flat * target_flat).sum(dim=1)\n",
        "        FN = ((1 - pred_flat) * target_flat).sum(dim=1)\n",
        "        FP = (pred_flat * (1 - target_flat)).sum(dim=1)\n",
        "\n",
        "        # Tversky index\n",
        "        tversky = (TP + self.smooth) / (TP + self.alpha * FN + self.beta * FP + self.smooth)\n",
        "\n",
        "        return 1 - tversky.mean()\n",
        "\n",
        "class LovaszLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Make sure target has same shape as predictions\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        # Handle batch dimension - process each sample separately\n",
        "        batch_size = pred.size(0)\n",
        "        losses = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            pred_sample = pred[i].view(-1)\n",
        "            target_sample = target[i].view(-1)\n",
        "\n",
        "            # Sort predictions by error\n",
        "            errors = torch.abs(target_sample - pred_sample)\n",
        "            perm = torch.argsort(errors, dim=0, descending=True)\n",
        "            pred_sorted = pred_sample[perm]\n",
        "            target_sorted = target_sample[perm]\n",
        "\n",
        "            # Calculate IoU at each threshold\n",
        "            intersection = target_sorted.cumsum(0)\n",
        "            union = target_sorted.sum() + (1 - target_sorted).cumsum(0)\n",
        "            iou = 1 - (intersection / (union + 1e-5))\n",
        "\n",
        "            # Get gradients\n",
        "            grad = torch.cat((torch.ones_like(iou[:1]), iou[1:] - iou[:-1]))\n",
        "\n",
        "            # Calculate final loss\n",
        "            sample_loss = (grad * errors[perm]).sum()\n",
        "            losses.append(sample_loss)\n",
        "\n",
        "        return torch.stack(losses).mean()\n",
        "\n",
        "# Generalized IoU Loss\n",
        "class GeneralizedIoULoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-5):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Make sure target has same shape as predictions\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        # Handle batch dimension properly\n",
        "        batch_size = pred.size(0)\n",
        "        pred_flat = pred.view(batch_size, -1)\n",
        "        target_flat = target.view(batch_size, -1)\n",
        "\n",
        "        # Calculate intersection and union per batch item\n",
        "        intersection = (pred_flat * target_flat).sum(dim=1)\n",
        "        union = pred_flat.sum(dim=1) + target_flat.sum(dim=1) - intersection\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
        "\n",
        "        # Calculate the enclosing box (in this 1D case, it's just the max of pred and target)\n",
        "        enclosed_area = torch.max(pred_flat, target_flat).sum(dim=1)\n",
        "\n",
        "        # Calculate GIoU\n",
        "        giou = iou - (enclosed_area - union) / (enclosed_area + self.smooth)\n",
        "\n",
        "        return 1 - giou.mean()\n",
        "\n",
        "# Combined Loss Function\n",
        "class CombinedSegLoss(nn.Module):\n",
        "    def __init__(self, dice_weight=0.3, focal_weight=0.2, tversky_weight=0.2,\n",
        "                 lovasz_weight=0.15, giou_weight=0.15, wce_weight=0.0, pos_weight=10.0):\n",
        "        super().__init__()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.tversky_weight = tversky_weight\n",
        "        self.lovasz_weight = lovasz_weight\n",
        "        self.giou_weight = giou_weight\n",
        "        self.wce_weight = wce_weight\n",
        "\n",
        "        # Initialize individual losses\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.focal_loss = FocalLoss(alpha=0.9, gamma=3.5)\n",
        "        self.tversky_loss = TverskyLoss(alpha=0.7, beta=0.3)\n",
        "        self.lovasz_loss = LovaszLoss()\n",
        "        self.giou_loss = GeneralizedIoULoss()\n",
        "        self.wce_loss = WeightedCrossEntropyLoss(pos_weight=pos_weight)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Ensure target has same shape as predictions\n",
        "        if pred.dim() == 4 and target.dim() == 3:\n",
        "            target = target.unsqueeze(1)\n",
        "\n",
        "        # Make sure spatial dimensions match\n",
        "        if pred.shape[2:] != target.shape[2:]:\n",
        "            target = F.interpolate(target, size=pred.shape[2:], mode='nearest')\n",
        "\n",
        "        losses = {\n",
        "            'dice': self.dice_loss(pred, target) * self.dice_weight,\n",
        "            'focal': self.focal_loss(pred, target) * self.focal_weight,\n",
        "            'tversky': self.tversky_loss(pred, target) * self.tversky_weight,\n",
        "            'lovasz': self.lovasz_loss(pred, target) * self.lovasz_weight,\n",
        "            'giou': self.giou_loss(pred, target) * self.giou_weight\n",
        "        }\n",
        "\n",
        "        # add WCE if the weight is non-zero\n",
        "        if self.wce_weight > 0:\n",
        "            losses['wce'] = self.wce_loss(pred, target) * self.wce_weight\n",
        "\n",
        "        # Return the sum of all losses\n",
        "        return sum(losses.values())\n",
        "\n",
        "# Model training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=50, device='cuda', save_dir='models'):\n",
        "    # Create directory for saving model\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize best model and metrics\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    best_dice = 0.0\n",
        "\n",
        "    # Initialize history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_dice': []\n",
        "    }\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc='Training', unit='batch', leave=False)\n",
        "        for inputs, targets in train_bar:\n",
        "            try:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "\n",
        "                    # Backward pass and optimize\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Update statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                batch_count += 1\n",
        "                train_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if batch_count > 0:\n",
        "            epoch_train_loss = running_loss / (batch_count * train_loader.batch_size)\n",
        "            history['train_loss'].append(epoch_train_loss)\n",
        "        else:\n",
        "            print(\"Warning: No valid batches in this epoch\")\n",
        "            epoch_train_loss = float('inf')\n",
        "            history['train_loss'].append(epoch_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_dice = 0.0\n",
        "        val_batch_count = 0\n",
        "\n",
        "        val_bar = tqdm(val_loader, desc='Validation', unit='batch', leave=False)\n",
        "        for inputs, targets in val_bar:\n",
        "            try:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                    # Ensure outputs have the same spatial dimensions as targets\n",
        "                    if outputs.shape[2:] != targets.shape[1:]:  # targets might be [B, H, W]\n",
        "                        outputs = F.interpolate(outputs, size=targets.shape[1:], mode='bilinear', align_corners=True)\n",
        "\n",
        "                    loss = criterion(outputs, targets)\n",
        "\n",
        "                    # Ensure targets have same dimension as predictions\n",
        "                    if outputs.dim() == 4 and targets.dim() == 3:\n",
        "                        targets = targets.unsqueeze(1)\n",
        "\n",
        "                    # Calculate Dice score (for monitoring)\n",
        "                    preds = (outputs > 0.5).float()\n",
        "                    intersection = (preds * targets).sum()\n",
        "                    dice = (2.0 * intersection + 1e-5) / (preds.sum() + targets.sum() + 1e-5)\n",
        "\n",
        "                # Update statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_dice += dice.item() * inputs.size(0)\n",
        "                val_batch_count += 1\n",
        "                val_bar.set_postfix({'loss': loss.item(), 'dice': dice.item()})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in validation batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if val_batch_count > 0:\n",
        "            epoch_val_loss = running_loss / (val_batch_count * val_loader.batch_size)\n",
        "            epoch_val_dice = running_dice / (val_batch_count * val_loader.batch_size)\n",
        "            history['val_loss'].append(epoch_val_loss)\n",
        "            history['val_dice'].append(epoch_val_dice)\n",
        "        else:\n",
        "            print(\"Warning: No valid batches in validation\")\n",
        "            epoch_val_loss = float('inf')\n",
        "            epoch_val_dice = 0.0\n",
        "            history['val_loss'].append(epoch_val_loss)\n",
        "            history['val_dice'].append(epoch_val_dice)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if epoch_val_dice > best_dice:\n",
        "            best_dice = epoch_val_dice\n",
        "            best_loss = epoch_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "                'dice': best_dice,\n",
        "                'history': history\n",
        "            }, os.path.join(save_dir, 'best_model.pth'))\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Train Loss: {epoch_train_loss:.4f}')\n",
        "        print(f'Validation Loss: {epoch_val_loss:.4f}, Dice: {epoch_val_dice:.4f}')\n",
        "        print(f'Best Validation Dice: {best_dice:.4f}')\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_val_loss,\n",
        "                'dice': epoch_val_dice,\n",
        "                'history': history\n",
        "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
        "\n",
        "    # Calculate total training time\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best validation Dice: {best_dice:.4f}')\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Model evaluation function\n",
        "def evaluate_model(model, test_loader, device='cuda'):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    running_dice = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_precision = 0.0\n",
        "    running_recall = 0.0\n",
        "    running_f1 = 0.0\n",
        "    running_lovasz = 0.0\n",
        "\n",
        "    lovasz_loss = LovaszLoss()\n",
        "    test_bar = tqdm(test_loader, desc='Evaluation')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_bar:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            preds = (outputs > 0.5).float()\n",
        "\n",
        "            # Calculate metrics\n",
        "            intersection = (preds * targets).sum().item()\n",
        "            union = preds.sum().item() + targets.sum().item() - intersection\n",
        "\n",
        "            # Avoid division by zero\n",
        "            if union > 0:\n",
        "                iou = intersection / union\n",
        "            else:\n",
        "                iou = 1.0\n",
        "\n",
        "            if preds.sum().item() > 0:\n",
        "                precision = intersection / preds.sum().item()\n",
        "            else:\n",
        "                precision = 0.0\n",
        "\n",
        "            if targets.sum().item() > 0:\n",
        "                recall = intersection / targets.sum().item()\n",
        "            else:\n",
        "                recall = 1.0\n",
        "\n",
        "            # F1 score\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0.0\n",
        "\n",
        "            # Dice score (same as F1 for binary segmentation)\n",
        "            dice = 2 * intersection / (preds.sum().item() + targets.sum().item() + 1e-5)\n",
        "\n",
        "            # Lovasz loss\n",
        "            lovasz = lovasz_loss(outputs, targets).item()\n",
        "\n",
        "            # Update statistics\n",
        "            running_dice += dice * inputs.size(0)\n",
        "            running_iou += iou * inputs.size(0)\n",
        "            running_precision += precision * inputs.size(0)\n",
        "            running_recall += recall * inputs.size(0)\n",
        "            running_f1 += f1 * inputs.size(0)\n",
        "            running_lovasz += lovasz * inputs.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            test_bar.set_postfix({\n",
        "                'Dice': dice,\n",
        "                'IoU': iou,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall\n",
        "            })\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_dice = running_dice / len(test_loader.dataset)\n",
        "    avg_iou = running_iou / len(test_loader.dataset)\n",
        "    avg_precision = running_precision / len(test_loader.dataset)\n",
        "    avg_recall = running_recall / len(test_loader.dataset)\n",
        "    avg_f1 = running_f1 / len(test_loader.dataset)\n",
        "    avg_lovasz = running_lovasz / len(test_loader.dataset)\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(\"\\nTest metrics:\")\n",
        "    print(f\"Dice score: {avg_dice:.4f}\")\n",
        "    print(f\"IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Recall: {avg_recall:.4f}\")\n",
        "    print(f\"F1 score: {avg_f1:.4f}\")\n",
        "    print(f\"Lovasz loss: {avg_lovasz:.4f}\")\n",
        "\n",
        "    # Save metrics to file\n",
        "    metrics = {\n",
        "        'dice': avg_dice,\n",
        "        'iou': avg_iou,\n",
        "        'precision': avg_precision,\n",
        "        'recall': avg_recall,\n",
        "        'f1': avg_f1,\n",
        "        'lovasz': avg_lovasz\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, image, device='cuda'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        output = model(image.unsqueeze(0))\n",
        "        pred = (output > 0.5).float()\n",
        "    return pred.squeeze(0).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJEeRq-QffIz"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define hyperparameters\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_EPOCHS = 60\n",
        "    LEARNING_RATE = 1e-4\n",
        "    USE_MULTISPECTRAL = True\n",
        "\n",
        "    # Set model output directory\n",
        "    model_dir = os.path.join(processed_dir, \"models\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # First, prepare the data and create datasets\n",
        "        split_paths = preprocess_data_for_training()\n",
        "        train_loader, val_loader, test_loader = prepare_data_loaders(\n",
        "            split_paths,\n",
        "            os.path.join(base_dir, \"patches\"),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            use_multispectral=USE_MULTISPECTRAL\n",
        "        )\n",
        "\n",
        "        # nOw we can access the dataset through the data loader\n",
        "        # Get a sample from the training dataset to determine input channels\n",
        "        sample_img = None\n",
        "        for sample_batch in train_loader:\n",
        "            sample_img, _ = sample_batch\n",
        "            in_channels = sample_img.shape[1]  # Shape is [batch_size, channels, height, width]\n",
        "            print(f\"Using {in_channels} input channels for the model\")\n",
        "            break\n",
        "\n",
        "        if sample_img is None:\n",
        "            raise ValueError(\"No valid samples found in the training dataset\")\n",
        "\n",
        "        # Initialize model with the correct number of input channels\n",
        "        model = UNeXt(in_channels=12, out_channels=1)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Initialize loss function\n",
        "        criterion = CombinedSegLoss(\n",
        "            dice_weight=0.3,\n",
        "            focal_weight=0.2,\n",
        "            tversky_weight=0.2,\n",
        "            lovasz_weight=0.2,\n",
        "            giou_weight=0.1,\n",
        "            wce_weight=0.0,\n",
        "            pos_weight=10.0  # I added pos_weight for class imbalance thinking it might help\n",
        "        )\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        # Move model to device safely\n",
        "        model = model.to(device)\n",
        "        print(\"Model successfully moved to device\")\n",
        "\n",
        "        # Train model\n",
        "        print(f\"\\nStarting model training for {NUM_EPOCHS} epochs...\")\n",
        "        model, history = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            device=device,\n",
        "            save_dir=model_dir\n",
        "        )\n",
        "\n",
        "        # Evaluate model on test set\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        metrics = evaluate_model(model, test_loader, device=device)\n",
        "\n",
        "        # Save metrics\n",
        "        with open(os.path.join(model_dir, \"test_metrics.json\"), 'w') as f:\n",
        "            import json\n",
        "            json.dump(metrics, f, indent=4)\n",
        "\n",
        "        print(\"\\nTraining and evaluation completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLIOYbd6ffFk"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'attention_unet_plastic_debris_pytorch.pth')\n",
        "print(\"PyTorch model trained and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3dkgN2Pfe_k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, test_loader, device='cuda'):\n",
        "    \"\"\"Evaluate model on test set and generate confusion matrix\"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    running_dice = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_precision = 0.0\n",
        "    running_recall = 0.0\n",
        "    running_f1 = 0.0\n",
        "    running_lovasz = 0.0\n",
        "\n",
        "    lovasz_loss = LovaszLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device).float()  # Ensure targets are float for loss calculations\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            preds = (outputs > 0.5).float()  # Threshold predictions at 0.5\n",
        "\n",
        "            # Flatten tensors for confusion matrix\n",
        "            y_true.extend(targets.cpu().numpy().flatten())\n",
        "            y_pred.extend(preds.cpu().numpy().flatten())\n",
        "\n",
        "            # Calculate metrics\n",
        "            intersection = (preds * targets).sum().item()\n",
        "            union = preds.sum().item() + targets.sum().item() - intersection\n",
        "\n",
        "            # Avoid division by zero\n",
        "            if union > 0:\n",
        "                iou = intersection / union\n",
        "            else:\n",
        "                iou = 1.0\n",
        "\n",
        "            if preds.sum().item() > 0:\n",
        "                precision = intersection / preds.sum().item()\n",
        "            else:\n",
        "                precision = 0.0\n",
        "\n",
        "            if targets.sum().item() > 0:\n",
        "                recall = intersection / targets.sum().item()\n",
        "            else:\n",
        "                recall = 1.0\n",
        "\n",
        "            # F1 score\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0.0\n",
        "\n",
        "            # Dice score (same as F1 for binary segmentation)\n",
        "            dice = 2 * intersection / (preds.sum().item() + targets.sum().item() + 1e-5)\n",
        "\n",
        "            # Lovasz loss\n",
        "            lovasz = lovasz_loss(outputs, targets).item()\n",
        "\n",
        "            # Update statistics\n",
        "            running_dice += dice * inputs.size(0)\n",
        "            running_iou += iou * inputs.size(0)\n",
        "            running_precision += precision * inputs.size(0)\n",
        "            running_recall += recall * inputs.size(0)\n",
        "            running_f1 += f1 * inputs.size(0)\n",
        "            running_lovasz += lovasz * inputs.size(0)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_dice = running_dice / len(test_loader.dataset)\n",
        "    avg_iou = running_iou / len(test_loader.dataset)\n",
        "    avg_precision = running_precision / len(test_loader.dataset)\n",
        "    avg_recall = running_recall / len(test_loader.dataset)\n",
        "    avg_f1 = running_f1 / len(test_loader.dataset)\n",
        "    avg_lovasz = running_lovasz / len(test_loader.dataset)\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(\"\\nTest metrics:\")\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Dice score: {avg_dice:.4f}\")\n",
        "    print(f\"IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Recall: {avg_recall:.4f}\")\n",
        "    print(f\"F1 score: {avg_f1:.4f}\")\n",
        "    print(f\"Lovasz loss: {avg_lovasz:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Non-Debris', 'Debris'],\n",
        "                yticklabels=['Non-Debris', 'Debris'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'dice': avg_dice,\n",
        "        'iou': avg_iou,\n",
        "        'precision': avg_precision,\n",
        "        'recall': avg_recall,\n",
        "        'f1': avg_f1,\n",
        "        'lovasz': avg_lovasz,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def visualize_predictions(model, test_loader, device='cuda', num_samples=5):\n",
        "    \"\"\"Visualize model predictions on test data\"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(test_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            preds = (outputs > 0.5).float()\n",
        "\n",
        "            # Move tensors to CPU for visualization\n",
        "            inputs = inputs.cpu().numpy()\n",
        "            targets = targets.cpu().numpy()\n",
        "            preds = preds.cpu().numpy()\n",
        "\n",
        "            # Plot input image (RGB bands)\n",
        "            axes[i, 0].imshow(inputs[0][:3].transpose(1, 2, 0))  # First 3 channels as RGB\n",
        "            axes[i, 0].set_title('Input Image')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # Plot ground truth\n",
        "            axes[i, 1].imshow(targets[0].squeeze(), cmap='gray')\n",
        "            axes[i, 1].set_title('Ground Truth')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            # Plot predictions\n",
        "            axes[i, 2].imshow(preds[0].squeeze(), cmap='gray')\n",
        "            axes[i, 2].set_title('Prediction')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "cm, accuracy = evaluate_model(model, test_loader, device='cuda')\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_predictions(model, test_loader, device=device, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSBVSQ3cdUNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}